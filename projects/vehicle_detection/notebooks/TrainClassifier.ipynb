{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('TrainClassifier')\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from vehicle_detector import extract_features\n",
    "from vehicle_detector.utils import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_from_dataset_name(dataset_name):\n",
    "    \n",
    "    tokens = dataset_name.split('_')\n",
    "    offset = 0\n",
    "    if tokens[0] == 'unfiltered':\n",
    "        offset = 1\n",
    "#     print(tokens)\n",
    "    params = {\n",
    "        'C': literal_eval(tokens[3 + offset]),\n",
    "        'gamma': tokens[5 + offset] if tokens[5 + offset]=='auto' else literal_eval(tokens[5 + offset]),\n",
    "        'color_hist': literal_eval(tokens[7 + offset]),\n",
    "        'orientations': literal_eval(tokens[9 + offset]),\n",
    "        'pixels_per_cell': literal_eval(tokens[11 + offset]),\n",
    "        'cells_per_block': literal_eval(tokens[13 + offset]),\n",
    "        'hog_color_space': os.path.splitext(tokens[15 + offset])[0]\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfiltered_A_99.125_C_10_gamma_auto_CH_False_O_10_P_(16, 16)_C_(2, 2)_CS_BGR2HSV.cpickle 98.72\n",
      "unfiltered_A_99.125_C_10_gamma_auto_CH_True_O_10_P_(16, 16)_C_(4, 4)_CS_BGR2YCrCb.cpickle 99.556\n",
      "A_99.0_C_10_gamma_auto_CH_True_O_10_P_(8, 8)_C_(4, 4)_CS_BGR2HSV.cpickle 99.199\n",
      "A_99.5_C_10_gamma_auto_CH_True_O_12_P_(16, 16)_C_(4, 4)_CS_BGR2YCrCb.cpickle 99.545\n",
      "A_99.0_C_10_gamma_auto_CH_True_O_10_P_(8, 8)_C_(4, 4)_CS_BGR2YCrCb.cpickle 99.272\n",
      "A_99.125_C_10_gamma_auto_CH_False_O_10_P_(16, 16)_C_(2, 2)_CS_BGR2HSV.cpickle 98.854\n",
      "unfiltered_A_99.5_C_10_gamma_auto_CH_True_O_12_P_(16, 16)_C_(4, 4)_CS_BGR2YCrCb.cpickle 99.573\n",
      "unfiltered_A_99.0_C_10_gamma_auto_CH_True_O_10_P_(8, 8)_C_(4, 4)_CS_BGR2HSV.cpickle 98.908\n",
      "A_99.125_C_10_gamma_auto_CH_True_O_9_P_(16, 16)_C_(4, 4)_CS_BGR2YCrCb.cpickle 99.691\n",
      "unfiltered_A_99.125_C_10_gamma_auto_CH_False_O_9_P_(16, 16)_C_(2, 2)_CS_BGR2YCrCb.cpickle 98.925\n",
      "unfiltered_A_99.125_C_10_gamma_auto_CH_True_O_12_P_(16, 16)_C_(2, 2)_CS_BGR2HSV.cpickle 99.266\n",
      "A_99.0_C_10_gamma_auto_CH_True_O_9_P_(8, 8)_C_(4, 4)_CS_BGR.cpickle 98.581\n",
      "A_99.125_C_10_gamma_auto_CH_True_O_10_P_(16, 16)_C_(4, 4)_CS_BGR2YCrCb.cpickle 99.582\n",
      "unfiltered_A_99.0_C_10_gamma_auto_CH_True_O_9_P_(8, 8)_C_(2, 2)_CS_BGR2YCrCb.cpickle 98.959\n",
      "A_99.125_C_10_gamma_auto_CH_False_O_9_P_(16, 16)_C_(2, 2)_CS_BGR2YCrCb.cpickle 99.036\n",
      "A_99.125_C_10_gamma_auto_CH_True_O_12_P_(16, 16)_C_(2, 2)_CS_BGR2HSV.cpickle 99.254\n",
      "unfiltered_A_99.0_C_10_gamma_auto_CH_True_O_9_P_(8, 8)_C_(4, 4)_CS_BGR.cpickle 98.516\n",
      "unfiltered_A_99.375_C_10_gamma_auto_CH_True_O_10_P_(16, 16)_C_(4, 4)_CS_BGR.cpickle 99.266\n",
      "unfiltered_A_99.0_C_10_gamma_auto_CH_True_O_10_P_(8, 8)_C_(4, 4)_CS_BGR2YCrCb.cpickle 99.249\n",
      "unfiltered_A_99.125_C_10_gamma_auto_CH_True_O_9_P_(16, 16)_C_(4, 4)_CS_BGR2YCrCb.cpickle 99.42\n",
      "A_99.375_C_10_gamma_auto_CH_True_O_10_P_(16, 16)_C_(4, 4)_CS_BGR.cpickle 99.145\n",
      "A_99.0_C_10_gamma_auto_CH_True_O_9_P_(8, 8)_C_(2, 2)_CS_BGR2YCrCb.cpickle 99.108\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = '../data/datasets'\n",
    "datasets = list(filter(lambda x: x.endswith('.hdf5') == True, os.listdir(dataset_dir)))\n",
    "models_dir = '../data/models'\n",
    "\n",
    "models = []\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "for this_dataset in datasets:\n",
    "    X, y = dataset.load_dataset(os.path.join(dataset_dir, this_dataset), 'carnd_p5')\n",
    "    rand_state = np.random.randint(0,100)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,\n",
    "                                                        random_state=rand_state)\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    \n",
    "    # Apply the scaler to X\n",
    "    X_train = X_scaler.transform(X_train)\n",
    "    X_test = X_scaler.transform(X_test)\n",
    "    params = get_params_from_dataset_name(this_dataset)\n",
    "    \n",
    "    clf = svm.SVC(kernel='rbf', C=params['C'], gamma=params['gamma'],\n",
    "                  probability=True, random_state=rand_state)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    model_name = '{name}.cpickle'.format(name=os.path.splitext(this_dataset)[0])\n",
    "    scaler_name = '{name}_scaler.cpickle'.format(name=os.path.splitext(this_dataset)[0])\n",
    "    model_filepath = os.path.join(models_dir,model_name)\n",
    "    scaler_filepath = os.path.join(models_dir, scaler_name)\n",
    "    \n",
    "    with open(model_filepath, 'wb') as fp:\n",
    "        fp.write(pickle.dumps(clf))\n",
    "    with open(scaler_filepath, 'wb') as fp:\n",
    "        fp.write(pickle.dumps(X_scaler))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = round(accuracy_score(y_test, y_pred) * 100, 3)\n",
    "    print(model_name, accuracy)\n",
    "    \n",
    "    models.append({\n",
    "        'name': model_name,\n",
    "        'file_path': model_filepath,\n",
    "        'accuracy': accuracy,\n",
    "        'scaler_file_path': scaler_filepath\n",
    "    })\n",
    "    \n",
    "with open('../data/top_models.json', 'w') as fp:\n",
    "    json.dump(models, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
